{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55f40a76-eeac-4f84-aca3-c95976e0a9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    1\n",
      "Purity: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorizer with Preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Original documents\n",
    "dataset = [\n",
    "    \"I love playing football on the weekends\",\n",
    "    \"I enjoy hiking and camping in the mountains\",\n",
    "    \"I like to read books and watch movies\",\n",
    "    \"I prefer playing video games over sports\",\n",
    "    \"I love listening to music and going to concerts\"\n",
    "]\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessed_dataset = [preprocess(doc) for doc in dataset]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_dataset)\n",
    "\n",
    "# Clustering\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display results\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Purity calculation\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da5fb007-a82e-43dc-8b4b-dbae3886a059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n",
      "Purity: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec Vectorizer with Preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "\n",
    "# Dataset and preprocessing\n",
    "dataset = [\n",
    "    \"I love playing football on the weekends\",\n",
    "    \"I enjoy hiking and camping in the mountains\",\n",
    "    \"I like to read books and watch movies\",\n",
    "    \"I prefer playing video games over sports\",\n",
    "    \"I love listening to music and going to concerts\"\n",
    "]\n",
    "\n",
    "tokenized_dataset = [preprocess_tokens(doc) for doc in dataset]\n",
    "\n",
    "# Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Document vectors\n",
    "X = np.array([\n",
    "    np.mean([word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv], axis=0)\n",
    "    for tokens in tokenized_dataset\n",
    "])\n",
    "\n",
    "# Clustering\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display results\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Purity calculation\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fbc6ecb4-3a00-4a86-8974-422a0dcb0ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top TF-IDF terms per cluster:\n",
      "Cluster 0:\n",
      "  service\n",
      "  customer\n",
      "  since\n",
      "  adding\n",
      "  boxes\n",
      "  second\n",
      "  speed\n",
      "  protocol\n",
      "  investigating\n",
      "  malfunction\n",
      "\n",
      "Cluster 1:\n",
      "  internet\n",
      "  comcast\n",
      "  mbps\n",
      "  service\n",
      "  would\n",
      "  day\n",
      "  xfinity\n",
      "  contract\n",
      "  call\n",
      "  get\n",
      "\n",
      "Cluster 2:\n",
      "  internet\n",
      "  rude\n",
      "  contract\n",
      "  years\n",
      "  comcast\n",
      "  joke\n",
      "  extra\n",
      "  10\n",
      "  local\n",
      "  im\n",
      "\n",
      "Purity (TF-IDF): 1.00\n",
      "Purity (Word2Vec): 1.00\n",
      "                                                text  cluster_tfidf  \\\n",
      "0  I used to love Comcast. Until all these consta...              1   \n",
      "1  I'm so over Comcast! The worst internet provid...              2   \n",
      "2  If I could give them a negative star or no sta...              0   \n",
      "3  I've had the worst experiences so far since in...              0   \n",
      "4  Check your contract when you sign up for Comca...              2   \n",
      "\n",
      "   cluster_w2v  \n",
      "0            1  \n",
      "1            2  \n",
      "2            2  \n",
      "3            2  \n",
      "4            1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "# Basic stopwords list\n",
    "basic_stopwords = set([\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
    "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "    'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
    "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
    "    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "    'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    "    'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n",
    "    'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just',\n",
    "    'don', 'should', 'now'\n",
    "])\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in basic_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Purity calculation function\n",
    "def calculate_purity(y_true):\n",
    "    counter = Counter(y_true)\n",
    "    return sum(counter.values()) / len(y_true)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('customer_complaints_1.csv')\n",
    "texts = df['text'].dropna().tolist()\n",
    "preprocessed_texts = [preprocess_text(t) for t in texts]\n",
    "\n",
    "### TF-IDF CLUSTERING\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(preprocessed_texts)\n",
    "\n",
    "k = 3\n",
    "km_tfidf = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred_tfidf = km_tfidf.fit_predict(X_tfidf)\n",
    "df['cluster_tfidf'] = y_pred_tfidf\n",
    "\n",
    "print(\"Top TF-IDF terms per cluster:\")\n",
    "order_centroids = km_tfidf.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\"  {terms[ind]}\")\n",
    "    print()\n",
    "\n",
    "purity_tfidf = calculate_purity(y_pred_tfidf)\n",
    "print(f\"Purity (TF-IDF): {purity_tfidf:.2f}\")\n",
    "\n",
    "### WORD2VEC CLUSTERING\n",
    "tokenized_texts = [t.split() for t in preprocessed_texts]\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Average vector for each document\n",
    "X_w2v = np.array([\n",
    "    np.mean([w2v_model.wv[word] for word in words if word in w2v_model.wv], axis=0)\n",
    "    for words in tokenized_texts\n",
    "])\n",
    "\n",
    "km_w2v = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred_w2v = km_w2v.fit_predict(X_w2v)\n",
    "df['cluster_w2v'] = y_pred_w2v\n",
    "\n",
    "purity_w2v = calculate_purity(y_pred_w2v)\n",
    "print(f\"Purity (Word2Vec): {purity_w2v:.2f}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(df[['text', 'cluster_tfidf', 'cluster_w2v']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1524c2-3d86-4dab-a296-5813ffa8e378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
